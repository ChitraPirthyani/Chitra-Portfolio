# Hi! I am **_Chitra_**.

**Check out my work.**

# [Project 1: Time-Series Forecasting of ComEd Energy Consumption Data](https://github.com/ChitraPirthyani/DataNerd/blob/master/Time%20series%20Analysis-%20ComEd.R)
* **Data Pre-processing:** Cleaned ComEd Consumption dataset and checked if it was stationary, normal and seasonal.
Built hourly, daily and monthly models after chaning it to a ts() data sequence.
* **Exploratory Analysis:** Performed exploratory analysis including histograms, time plots, ACF and PACF plots, ADF tests.
* **Model Building:** Created six models using ARIMA, SARIMA and regression analysis. 
* Applied Model Diagnostic techniques such as: Residual Analysis, Ljung Box test, ACF ofresiduals, backtesting, BIC , MAPE and RMSE - to test model performance. 
* **Forecasting:** Finally, performed forecasting trend analysis on the best performing model. 

![](/images/Picture1.png)

# [Project 2: Classification & Regression of Weather Data](https://github.com/ChitraPirthyani/DataNerd/blob/master/DSC441%20Weather%20data%20Decision%20Trees.R)
* **Data Pre-processing:** Cleaned and pre-processed weather data with 6 million data points to arrive at 2 million data points.
* **Exploratory Analysis:** Created descriptive plots including corr-plots, box plots and histograms.
* **Model Building:** Built models based on Simple Linear Regression, Multiple Regression and Classification.
* **Classification:** Created a Decision Tree and a confusion matirx to classify when the Sky is Over-cast, vs when it is not. 

![](/images/Picture3.png)


# [Project 3: Twitter API](https://github.com/ChitraPirthyani/DataNerd/blob/master/TwitterAPI.py)
* Accessed Twitter API to pull specific tweets with specific hashtags and keywords.
* Converted these tweets to a pandas dataframe and transported it to a csv file. 

# [Project 4: Advanced Machine Learning on Breast Cancer Data](https://github.com/ChitraPirthyani/DataNerd/blob/master/Advanced%20Machine%20Learning%20-%201.ipynb)
* **Data Pre-processing:** Data cleaning and train-test splits.
* **Classification Models:** 1. Random Forest Classification
                            2. Gradient Boosting Classification
                           3. GridSearch CV Model
* **Metric Analysis :** Compared all classification models using the following metrics : ROC Curves, Precision Recall Curves, Confusion Matrix, Accuracy Scores

![](/images/picture5.png)

# [Project 5: Regression Modeling of Avocado Prices in Chicago](https://github.com/ChitraPirthyani/DataNerd/blob/master/Avocado%20-%20v2.R)
* **Feature Analysis:** Applied correlation analysis between features and did exploratory analysis such as histograms, box plots.
* **Modeling:** For regression modeling, I created five models: 
                1. Full model
                2. Forward and backward testing models
                3. Stepwise testing models
                4. Full interaction model
                5. Interaction model with backward testing
* **Model Diagnosis:** Measured model performance based on metrics like Co-efficient tests, residual tests, RMSE, R-squared, MAE, QQ-plots. 

![](/images/picture8.png)

# [Project 6: Classification of Customer Churn](https://github.com/ChitraPirthyani/DataNerd/blob/master/Classification.R)
* **Description:** This project aims to determine which variables affect the customer's decision to Churn or Not Churn from a tele-communication service through various classification algorithms.
* **Modeling:** I built four models: 1. Logistic Regression model, 2. Linear Discriminant Analysis, 3. Principal Component Analysis, 4. Random Forest Classification.

![](/images/picture7.png)

# [Project 7: Web-scraping and NLP](https://github.com/ChitraPirthyani/DataNerd/blob/master/Flooring_Text.py)
* **Description:** For this internship project, I created a web scraper to extract header, p-tags and descriptive information from each website on a google-search
result page. 
* Performed ETL using :  Beautiful soup, requests, NLTK, pandas, NumPy
* Generated a data frame that contained URLs from industry-specific google searches, their corresponding meta tags and most
frequently occurring words and phrases. This data frame was used to improve SEO and keyword optimization for clients. 


